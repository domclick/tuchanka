#!/bin/bash
# safe bash
set -o errexit -o noclobber -o nounset -o pipefail
# get config
readonly script_dir="$(dirname "${BASH_SOURCE[0]}")"
. "${script_dir}/config.bash"
. "${script_dir}/hard_off.bash"
. "${script_dir}/rollback.bash"
. "${script_dir}/vm_ssh.bash"
. "${script_dir}/start_vms.bash"
. "${script_dir}/soft_off.bash"
. "${script_dir}/snapshot.bash"
readonly pgsql_dir='/var/lib/pgsql' # в ../common/postgresql.conf независимо прописан путь /var/lib/pgsql
hard_off
rollback 'install_soft'
start_vms
# common things for all VM
# Раскладывание общих файлов по всем VM, с разделением witness/все остальные
for h in "${vm_hostname[@]}"
do
	# add hosts to /etc/hosts
	vm_ssh "$h" 'cat >>/etc/hosts' <"${hosts}"
	# softdog
	vm_cp "$h" "${common_dir}/watchdog.conf" '/etc/modules-load.d/watchdog.conf'
	vm_ssh "$h" 'systemctl restart systemd-modules-load'
	# for pacemaker
	echo -e "${hacluster_password}\n${hacluster_password}" | vm_ssh "$h" 'passwd --stdin hacluster'
	# pacemaker redhat client, REST daemon
	vm_ssh "$h" 'systemctl start pcsd.service'
	vm_ssh "$h" 'systemctl enable pcsd.service'
	# bash
	vm_ssh "$h" 'cat >>.bash_profile' <"${common_dir}/bash_profile"
	vm_cp "$h" "${script_dir}/../${h}/bashrc_local" '.bashrc_local'
	vm_ssh "$h" 'cat >>.bashrc' <"${common_dir}/bashrc"
	# PostgreSQL and Witness specific things
	if [ "$h" = "${vm_hostname[${Witness}]}" ]
	then # witness
		vm_ssh "$h" 'pcs qdevice setup model net --enable --start'
		# manual check: pcs qdevice status net
	else # other, but not witness
		# PATH for PostgreSQL bin
		vm_cp "$h" "${common_dir}/profile.sh" '/etc/profile.d/tuchanka.sh'
		# PostgreSQL passwords
		vm_ssh "$h" 'umask 0177 && cat >~/.pgpass' <"${common_dir}/pgpass"
		vm_cp2pgsql "$h" "${common_dir}/pgpass" "${pgsql_dir}/.pgpass"
		# common PostgreSQL configs
		vm_cp2pgsql "$h" "${common_dir}/postgresql.conf" "${pgsql_dir}/postgresql.conf"
		vm_cp2pgsql "$h" "${common_dir}/pg_ident.conf" "${pgsql_dir}/pg_ident.conf"
		vm_cp2pgsql "$h" "${common_dir}/pg_hba.conf" "${pgsql_dir}/pg_hba.conf"
		# pacemaker monitor script
		vm_ssh "$h" 'mkdir -p bin'
		vm_cp "$h" "${common_dir}/mon" 'bin/mon'
		vm_ssh "$h" 'chmod 0755 bin/mon'
		# psql user config
		vm_cp "$h" "${common_dir}/psqlrc" '.psqlrc'
	fi
done
# цикл по кластерам, первичная настройка
for k in "${!krogan_cluster[@]}"
do
	krogan="${krogan_cluster[$k]}"
	# Нужен список всех hostname из кластера, использую костыль, но тут сработает
	hostnames=""
	for h in ${db_master[$k]} ${db_slaves[$k]}
	do
		hostnames="${hostnames} ${vm_hostname[$h]}"
	done
	# Нужен любой hostname из кластера для ssh доступа (настройка всех машин через одну)
	h="${vm_hostname[${db_master[$k]}]}"
	vm_ssh "$h" "pcs cluster auth ${vm_hostname[$Witness]} ${hostnames} -u hacluster -p '${hacluster_password}'"
	vm_ssh "$h" "pcs cluster setup --name ${krogan} ${hostnames}"
#	vm_ssh "$h" 'pcs stonith sbd enable'
	# В redhat работают .... Включение sbd автоматичесий включает auto_tie_breaker, который конфликтует и не нужен в случае quorum device.
	# Поэтому строчкой ниже workaround.
#	vm_ssh "$h" 'pcs quorum update auto_tie_breaker=0 --force'
	vm_ssh "$h" 'pcs cluster start --all'
	vm_ssh "$h" "pcs quorum device add model net host='${vm_hostname[$Witness]}' algorithm=ffsplit"
	# manual check: pcs quorum device status
	vm_ssh "$h" 'pcs property set stonith-enabled=false'
done

# Цикл по DB
for k in "${!db_master[@]}"
do
	# config_name используется в именах конфигов, соотвествует имени float_ip соответствующего мастеру этой DB
	config_name="${float_hostname[$k]}" pgport="${db_port[$k]}"
	master="${vm_hostname[${db_master[$k]}]}"
	pgdata="${pgsql_dir}/${config_name}" custom_conf="${config_name}.conf" restore_paf="${config_name}.paf" pacemaker_pcs="${config_name}.pcs"
	restore_sh="restore${config_name#'krogan'}"
	echo "Configure ${config_name}"
	vm_ssh "$master" "su -l postgres -c \"pg_ctl init --pgdata='${pgdata}' --options='--locale=ru_RU.UTF-8 --lc-messages=en_US.UTF-8 --auth-local=peer --auth-host=scram-sha-256'\""
	# pg_ident.conf pg_hba.conf postgresql.conf common for all databases
	vm_ssh "$master" "cp -a '${pgdata}/postgresql.conf' '${pgdata}/postgresql.conf.original'"
	vm_ssh "$master" "echo -e \"include='postgresql.conf.original'\ninclude_if_exists='../postgresql.conf'\ninclude_if_exists='../${custom_conf}'\" >|'${pgdata}/postgresql.conf'"
	vm_cp2pgsql "$master" "${script_dir}/../$master/${custom_conf}" "${pgsql_dir}/${custom_conf}"
	vm_cp2pgsql "$master" "${script_dir}/../$master/${restore_paf}" "${pgsql_dir}/${restore_paf}"
	# without -t is freezed here
	vm_ssh -t "$master" "su -l postgres -c \"pg_ctl --options='--synchronous_standby_names=' --pgdata='${pgdata}' start\""
	# создаю пользователя для репликации, пароль захардкожен, возможно в будущем будет другой способ аутентикации
	vm_ssh "$master" "psql postgres postgres --port=${pgport} --command=\"create user replicant with password 'Nexus 6' replication\""
	for s in ${db_slaves[$k]}
	do
		slave="${vm_hostname[$s]}"
		vm_cp2pgsql "$slave" "${script_dir}/../${slave}/${custom_conf}" "${pgsql_dir}/${custom_conf}"
		vm_cp2pgsql "$slave" "${script_dir}/../${slave}/${restore_paf}" "${pgsql_dir}/${restore_paf}"
		vm_ssh "$slave" "su -l postgres -c \"pg_basebackup --pgdata='${pgdata}' --dbname='host=${master} port=${pgport} user=replicant sslmode=disable' --progress --write-recovery-conf\""
		vm_ssh -t "$slave" "su -l postgres -c \"pg_ctl --pgdata='${pgdata}' start\""
		echo "Wait for starting replication on slave ${slave}"
		replication_status=$(vm_ssh "$slave" "psql --quiet --tuples-only --expanded --port=${pgport} postgres postgres --command=\"select status from pg_stat_wal_receiver where sender_host='${master}'\"")
		until [ "$replication_status" = 'status | streaming' ]
		do
			sleep 1
			replication_status=$(vm_ssh "$slave" "psql --quiet --tuples-only --expanded --port=${pgport} postgres postgres --command=\"select status from pg_stat_wal_receiver where sender_host='${master}'\"")
		done
	done
	# now stoping, master must be first
	for h in ${db_master[$k]} ${db_slaves[$k]}
	do
		hostname="${vm_hostname[$h]}"
		vm_ssh "$hostname" "su -l postgres -c \"pg_ctl --pgdata='${pgdata}' stop\""
		vm_cp "$hostname" "${pcs_dir}/${pacemaker_pcs}" "${pacemaker_pcs}"
		vm_ssh "$hostname" "cat >'bin/${restore_sh}'" <<-EOF
			#!/bin/sh
			# Временно, потом надо будет переделать чтобы был с бэкапом старой копии
			command rm -r '${pgdata}'
			command su -l postgres -c "pg_basebackup --pgdata='${pgdata}' --dbname='host=${config_name} port=${pgport} user=replicant sslmode=disable' --progress"
			EOF
		vm_ssh "$hostname" "chmod 755 'bin/${restore_sh}'"
	done
	# список команд для pcs для создания ресурса DB в кластере pacemaker
	vm_ssh "$master" ". '${pacemaker_pcs}'"
done

# цикл по кластерам, завершение перед выключением
for k in "${!krogan_cluster[@]}"
do
	# Нужен любой hostname из кластера для ssh доступа (настройка всех машин через одну)
	h="${vm_hostname[${db_master[$k]}]}"
	vm_ssh "$h" "pcs cluster stop --all"
done
soft_off
snapshot 'configure' 'The snapshot was taken at the end of the "configure" script.'
exit 0
