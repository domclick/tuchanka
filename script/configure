#!/bin/bash
# safe bash
set -o errexit -o noclobber -o nounset -o pipefail -o xtrace

# get config
readonly script_dir="$(dirname "${BASH_SOURCE[0]}")"
. "${script_dir}/load_config.bash"
. "${script_dir}/vm_ssh.bash"

if $autoVirtualBox
then
	. "${script_dir}/rollback.bash"
	. "${script_dir}/start_vms.bash"
	. "${script_dir}/snapshot.bash"
fi

readonly pgsql_dir='/var/lib/pgsql' # в ../common/postgresql.conf независимо прописан путь /var/lib/pgsql

if $autoVirtualBox
then
	rollback 'install_soft'
	start_vms
else
	echo 'При автоматической установке здесь происходит откат к snapshot, который был создан после доустановки пакетов.'
fi

# common things for all VM
# Раскладывание общих файлов по всем VM, с разделением witness/все остальные
for hk in "${!vm_hostname[@]}"
do
	h="${vm_hostname[$hk]}"
	# add hosts to /etc/hosts
	vm_ssh "$h" 'cat >>/etc/hosts' <"${hosts}"
	# softdog
	vm_cp "$h" "${common_dir}/watchdog.conf" '/etc/modules-load.d/watchdog.conf'
	vm_ssh "$h" 'systemctl restart systemd-modules-load'
	# Как его настраивать, чтобы работал совместо с pacemaker не знаю
	vm_ssh "$h" 'systemctl disable firewalld'
	vm_ssh "$h" 'systemctl stop firewalld'
	# Из-за ограничений VirtualBox пришлось сделать две подсетки:
	# одна для работы кластера на eth0
	# другая для выхода в Интернет через NAT на eth1 (default gw)
	# Запрещаю доступ к рабочей подсетке на eth0 через подсеть для выхода в Интернет на eth1 (через default gw)
	vm_ssh "$h" "cat >>'/etc/rc.local'" <<-EOF

		# Added by tuchanka configure script
		ip route add prohibit ${vboxnet_prefix}.0/24 protocol static metric 200
		EOF
	vm_ssh "$h" "chmod +x '/etc/rc.local'"
	# for pacemaker
	echo -e "${hacluster_password}\n${hacluster_password}" | vm_ssh "$h" 'passwd --stdin hacluster'
	# pacemaker redhat client, REST daemon
	vm_ssh "$h" 'systemctl start pcsd.service'
	vm_ssh "$h" 'systemctl enable pcsd.service'
	# bash
	vm_ssh "$h" 'cat >>.bash_profile' <"${common_dir}/bash_profile"
	vm_cp "$h" "${script_dir}/../${h}/bashrc_local" '.bashrc_local'
	vm_ssh "$h" 'cat >>.bashrc' <"${common_dir}/bashrc"
	# PostgreSQL and Witness specific things
	if [ "$h" = "${vm_hostname[${Witness}]}" ]
	then # witness
		vm_ssh "$h" "sed -e '/^COROSYNC_QNETD_OPTIONS=\"\"$/s//COROSYNC_QNETD_OPTIONS=\"-S dpd_interval=1000\"/' -i '/etc/sysconfig/corosync-qnetd'"
		vm_ssh "$h" 'pcs qdevice setup model net --enable --start'
		# manual check of qdevice: pcs qdevice status net
		# ntpd setup
		# witness is our backup ntp server, usefull for isolated testbed
		vm_ssh "$h" "sed -e '/^\(server\)\s/s//#\1/' -i '/etc/ntp.conf'"
		vm_ssh "$h" "cat >>'/etc/ntp.conf'" <<-EOF
			# Added by tuchanka configure script
			restrict ${vboxnet_prefix}.0 mask 255.255.255.0 kod nomodify notrap
			pool ru.pool.ntp.org iburst preempt
			server ntp21.vniiftri.ru iburst
			#server ntp1.vniiftri.ru iburst
			#server ntp2.vniiftri.ru iburst
			#server ntp3.vniiftri.ru iburst
			#server ntp4.vniiftri.ru iburst
			# orphan mode
			tos orphan 10
			EOF
		vm_ssh "$h" 'systemctl enable ntpd.service'
		vm_ssh "$h" 'systemctl start ntpd.service'
	else # other, but not witness
		# какой-то идиот выставил этому файлу права на выполнение
		vm_ssh "$h" 'chmod a-x /var/lib/pgsql/.bash_profile'
		# настраиваю shell для postgres user
		vm_ssh "$h" "cat >'${pgsql_dir}/.pgsql_profile'" <<-EOF
			unset PGDATA
			PATH="/usr/pgsql-${postgresql_version}/bin:\${PATH}"
			EOF
		# PostgreSQL passwords
		vm_ssh "$h" 'umask 0177 && cat >~/.pgpass' <"${common_dir}/pgpass"
		vm_cp2pgsql "$h" "${common_dir}/pgpass" "${pgsql_dir}/.pgpass"
		# common PostgreSQL configs
		vm_cp2pgsql "$h" "${common_dir}/postgresql.conf" "${pgsql_dir}/postgresql.conf"
		vm_cp2pgsql "$h" "${common_dir}/pg_ident.conf" "${pgsql_dir}/pg_ident.conf"
		vm_cp2pgsql "$h" "${common_dir}/pg_hba.conf" "${pgsql_dir}/pg_hba.conf"
		# pacemaker monitor script
		vm_ssh "$h" 'mkdir -p bin'
		vm_cp "$h" "${common_dir}/mon" 'bin/mon'
		vm_ssh "$h" 'chmod 0755 bin/mon'
		# psql user config
		vm_cp "$h" "${common_dir}/psqlrc" '.psqlrc'
		# ntpd setup
		vm_ssh "$h" "sed -e '/^\(server\)\s/s//#\1/' -i '/etc/ntp.conf'"
		vm_ssh "$h" "cat >>'/etc/ntp.conf'" <<-EOF
			# Added by tuchanka configure script
			restrict ${vboxnet_prefix}.0 mask 255.255.255.0 kod nomodify notrap
			pool ru.pool.ntp.org iburst preempt
			server ntp21.vniiftri.ru iburst
			#server ntp1.vniiftri.ru iburst
			#server ntp2.vniiftri.ru iburst
			#server ntp3.vniiftri.ru iburst
			#server ntp4.vniiftri.ru iburst
			# orphan mode
			tos orphan 11
			# orphan parent
			server witness iburst
			EOF
		# for all in the group, but not I
		for hhk in "${!vm_hostname[@]}"
		do
			if [ $hhk -ne $hk -a "${vm_groups[$hhk]}" = "${vm_groups[$hk]}" ]
			then
				vm_ssh "$h" "echo 'peer ${vm_hostname[$hhk]} xleave' >>'/etc/ntp.conf'"
			fi
		done
		vm_ssh "$h" 'systemctl enable ntpd.service'
		vm_ssh "$h" 'systemctl start ntpd.service'
	fi
done
# цикл по кластерам, первичная настройка
for k in "${!krogan_cluster[@]}"
do
	krogan="${krogan_cluster[$k]}"
	# Нужен список всех hostname из кластера, использую костыль, но тут сработает
	hostnames=""
	for h in ${db_master[$k]} ${db_slaves[$k]}
	do
		hostnames="${hostnames} ${vm_hostname[$h]}"
	done
	# Нужен любой hostname из кластера для ssh доступа (настройка всех машин через одну)
	h="${vm_hostname[${db_master[$k]}]}"
	vm_ssh "$h" "pcs cluster auth ${vm_hostname[$Witness]} ${hostnames} -u hacluster -p '${hacluster_password}'"

	# --wait-for-all поставил, так как есть проблемы со стартом tuchanka2 (кластер из 4х виртуалок)
	vm_ssh "$h" "pcs cluster setup --name ${krogan} --transport udp --encryption 1 ${hostnames} --wait_for_all 1"
	# Иногда при работе кластера, в том числе при cluster start --all, возникает отказ в работе.
	# Выглядит как CPU load 114, предполагаю, что при некоторых условиях pacemaker начинает
	# бесконечно размножать процессы. Надеюсь эта опция поможет решить эту проблему.
	# Не знаю, является ли эта проблема специфичной для VirtualBox или общая. 2 потому что на
	# виртуалках два виртуальных ядра. В реальности должно быть разумное значиние, например количество ядер
	# на сервере или около того. Поскольку устанавливать property можно только после поднятия кластера,
	# а проблему появляется иногда при поднятии кластера, выставляю эту опцию еще до поднятия кластера,
	# путем редактирования CIB файла напрямую.
	for hh in ${hostnames}
	do
		vm_ssh "$hh" "pcs -f '${cib}' property set node-action-limit=2"
	done
	vm_ssh "$h" 'pcs stonith sbd enable'
	# В redhat работают .... Включение sbd автоматичесий включает auto_tie_breaker, который конфликтует и не нужен в случае quorum device.
	# Поэтому строчкой ниже workaround.
	vm_ssh "$h" 'pcs quorum update auto_tie_breaker=0 --force'
	vm_ssh "$h" 'pcs cluster start --all'
	vm_ssh "$h" "pcs quorum device add sync_timeout=2000 timeout=1000 model net host='${vm_hostname[$Witness]}' algorithm=ffsplit"
	# manual check: pcs quorum device status
	vm_ssh "$h" 'pcs property set stonith-enabled=true'
	# Должно быть примерно в 2 раза больше, чем SBD_WATCHDOG_TIMEOUT в /etc/sysconfig/sbd в виртуалке
	vm_ssh "$h" 'pcs property set stonith-watchdog-timeout=10'
done

# Цикл по DB
for k in "${!db_master[@]}"
do
	# config_name используется в именах конфигов, соотвествует имени float_ip соответствующего мастеру этой DB
	config_name="${float_hostname[$k]}" pgport="${db_port[$k]}"
	master="${vm_hostname[${db_master[$k]}]}"
	pgdata="${pgsql_dir}/${config_name}" custom_conf="${config_name}.conf" restore_paf="${config_name}.paf" pacemaker_script="${config_name}"
	restore_sh="restore${config_name#'krogan'}"
	echo "Configure ${config_name}"
	vm_ssh "$master" "su --login postgres -c \"pg_ctl init --pgdata='${pgdata}' --options='--locale=ru_RU.UTF-8 --lc-messages=en_US.UTF-8 --auth-local=peer --auth-host=scram-sha-256'\""
	# pg_ident.conf pg_hba.conf postgresql.conf common for all databases
	vm_ssh "$master" "cp -a '${pgdata}/postgresql.conf' '${pgdata}/postgresql.conf.original'"
	vm_ssh "$master" "echo -e \"include='postgresql.conf.original'\ninclude_if_exists='../postgresql.conf'\ninclude_if_exists='../${custom_conf}'\" >|'${pgdata}/postgresql.conf'"
	vm_cp2pgsql "$master" "${script_dir}/../$master/${custom_conf}" "${pgsql_dir}/${custom_conf}"
	vm_cp2pgsql "$master" "${script_dir}/../$master/${restore_paf}" "${pgsql_dir}/${restore_paf}"
	# without -t is freezed here
	vm_ssh -t "$master" "su -l postgres -c \"pg_ctl --options='--synchronous_standby_names=' --pgdata='${pgdata}' start\""
	# создаю пользователя для репликации, пароль захардкожен, возможно в будущем будет другой способ аутентикации
	vm_ssh "$master" "psql postgres postgres --port=${pgport} --command=\"create user replicant with password 'Nexus 6' replication\""
	for s in ${db_slaves[$k]}
	do
		slave="${vm_hostname[$s]}"
		vm_cp2pgsql "$slave" "${script_dir}/../${slave}/${custom_conf}" "${pgsql_dir}/${custom_conf}"
		vm_cp2pgsql "$slave" "${script_dir}/../${slave}/${restore_paf}" "${pgsql_dir}/${restore_paf}"
		vm_ssh "$slave" "su -l postgres -c \"pg_basebackup --pgdata='${pgdata}' --dbname='host=${master} port=${pgport} user=replicant sslmode=disable' --progress --write-recovery-conf\""
		vm_ssh -t "$slave" "su -l postgres -c \"pg_ctl --pgdata='${pgdata}' start\""
		echo "Wait for starting replication on slave ${slave}"
		replication_status=$(vm_ssh "$slave" "psql --quiet --tuples-only --expanded --port=${pgport} postgres postgres --command=\"select status from pg_stat_wal_receiver where sender_host='${master}'\"")
		until [ "$replication_status" = 'status | streaming' ]
		do
			sleep 1
			replication_status=$(vm_ssh "$slave" "psql --quiet --tuples-only --expanded --port=${pgport} postgres postgres --command=\"select status from pg_stat_wal_receiver where sender_host='${master}'\"")
		done
	done
	# now stoping, master must be first
	for h in ${db_master[$k]} ${db_slaves[$k]}
	do
		hostname="${vm_hostname[$h]}"
		vm_ssh "$hostname" "su -l postgres -c \"pg_ctl --pgdata='${pgdata}' stop\""
		vm_cp "$hostname" "${pcs_dir}/${pacemaker_script}" "${pacemaker_script}"
		vm_ssh "$hostname" "cat >'bin/${restore_sh}'" <<-EOF
			#!/bin/sh
			# Временно, потом надо будет переделать чтобы был с бэкапом старой копии в S3
			command rm -r '${pgdata}'
			# Если бэкапить таким образом, заканчивается место в виртуалках
			#command mv '${pgdata}' "${pgdata}.\$(date --iso-8601=seconds)"
			command su -l postgres -c "pg_basebackup --pgdata='${pgdata}' --dbname='host=${config_name} port=${pgport} user=replicant sslmode=disable' --progress"
			EOF
		vm_ssh "$hostname" "chmod 755 'bin/${restore_sh}'"
		vm_cp "$hostname" "${script_dir}/../${hostname}/restore" "bin/"
	done
	# список команд для pcs для создания ресурса DB в кластере pacemaker
	vm_ssh "$master" "./'${pacemaker_script}'"
done

# цикл по кластерам, завершение перед выключением
for k in "${!krogan_cluster[@]}"
do
	# Нужен любой hostname из кластера для ssh доступа (настройка всех машин через одну)
	h="${vm_hostname[${db_master[$k]}]}"
	vm_ssh "$h" "pcs cluster stop --all"
done

if $autoVirtualBox
then
	snapshot 'configure' 'The snapshot was taken at the end of the "configure" script.'
else
	echo 'При автоматической установке здесь сохраняется snapshot с настроенным pacemaker.'
fi

exit 0
