![Krogan on Tuchanka](/images/krogan.png)
# ToDo
Пока нет, но должно появится в следующих итерациях:
- S3 архивирование (в том числе чтобы добирать недостающие страницы при репликации)
- Централизованая аутентификация пользователей PostgreSQL (в том числе логинов для репликации)
- Настройки для Zabbix
- Миграция БД между кластерами
- Апгрейд PostgreSQL (перенос всех БД на новый кластер)
- Общие сервисы для всех БД, например *pg_cron*.

# Введение
Это прототип отказоустойчивого кластера на базе Pacemaker и PostgreSQL для двух датацентров. Кластер разворачивается на виртуалках VirtualBox. Всего будет развернуто 9 виртуалок (суммарно 28GiB), которые образуют 3 отказоустойчивых кластера (разные варианты). Подробности нарисованы в [HAPgSQLStructure](HAPgSQLStructure.key). Первые два кластера состоят из двух серверов PostgreSQL, которые размещены в разных датацентрах и общего сервера *witness* c **quorum device** (размещенный на дешевой виртуалке в третьем датацентре), который разрешает неопределенность 50%/50% отдавая свой голос одной из сторон. Третий кластер состоит из четырех серверов PostgreSQL, по два на датацентр, один мастер, остальные реплики, выдерживает отказ двух серверов или одного датацентра. Это решение может быть, при необходимости, масштабировано на большее число реплик.

Сервис точного времени `ntpd` тоже перенастроен для отказоустойчивости, но там используются методы самого `ntpd` (*orphan mode*). Полная сетевая изоляция, конечно, маловероятна в продакшине, где каждый сервер кластера размещен в независимом датацентре, но возможна на этапе тестирвания, например, на ноутбуках. При полной сетевой изоляции кластера общий сервер *witness* начинает выполнять роль центрального ntp сервера раздавая своё время всем кластерам, тем самым синхронизируя все сервера между собой. В случае если и *witness* выйдет из строя/окажется изолированным, тогда своё время начнет раздавать один из серверов кластера (внутри кластера).

# Список файлов
#### README.md
Этот файл, краткое описание всего.

#### HAPgSQLStructure.key
Картинки для презентации. Открывается в Keynote.app (презентации под MacOs).

#### setup/
Bash скрипты для создания и удаления виртуалок.

#### pcs/
Шелловские скрипты, запускаться будут на виртуалках, содержат команды для `pcs`, которые описывают конфигурацию кластера.

#### heartbeat/
Скрипты, которые помогают тестировать отказоустойчивость кластера постоянно записывая и читая из специальной базы данных.

#### upload/
Там лежат файлы предназначеные для заливки на виртуалки.

#### upload/common/
Файлы общие для всех виртуалок.

#### test/
Скрипты которыми я тестировал кластер.

#### lib/
Библиотечка функций на *bash*, используются различными скриптами.

#### default_config.bash
По умолчанию используется конфиг *default_config.bash* из git, рабочий и достаточный. Но если в конфиг надо внести изменения, то надо скопировать *default_config.bash* в *config.bash* и его уже править.

# Установка
Для работы нужен VirtualBox не меньше **6.0.10**. На 6.0.8 были проблемы с *ACPI Shutdown*, которые приводили к зависанию процессов. С 5.x я не тестировал, но быть может работать будет.

Не удалось в VirtualBox настроить так, чтобы в одной подсетке был и доступ в Интернет и доступ хоста к виртуалкам. Поэтому в VirtualBox по два сетевых интерфейса в каждой виртуалке. Один сетевой интерфейс для связи машин внутри кластера и связи с хостом, второй для доступа в Интернет (сервера точного времени, DNS, пакеты из репозитория). В реальной системе можно будет сделать через один интерфейс, либо полностью задублировать рабочие подсети.

Для моего удобства процесс развертывания автоматизирован, в том числе скрипты автоматический запускают команды *VBoxManager* такие как: создание скиншотов файловой системы, запуск и остановка виртуалок. Поскольку процесс инсталяции с образа DVD и доустановка пакетов по сети занимает довольно длительное время, весь процесс установки осуществляется двумя этапами, их подробно опишу ниже, в конце каждого этапа автоматический создается снэпшот. А в начале этапа автоматический переходит на снэпшот предыдущего этапа.

Все команды для шелла даны для `bash`.

## Создание виртуальный машин и установка пакетов
Создаются виртуальные машины и устанавливаются пакеты командой:

	setup/install <redhat_installation_image.iso> <root_ssh_public_key.pub>

Загрузочный диск должен быть от редхатоподобного дистрибутива 7й версии. Я использовал `CentOS-7-x86_64-Minimal-1810.iso`. Не должно быть **'** в публичном ключе (он там может быть только в комментариях). И если приватный ключ от этого публичного ключа защищен паролем, он должен быть добавлен в keychain, например с помощью `ssh-add -K`. Ключи загружаются с помощью `ssh-add -A`, эту команду можно изменить в конфиге, например, для того чтобы загружать конкретный ключ, а не все, или чтобы не искать пароль в keychain. Скрипт делает:
- Создает вспомогательные конфигурационные файлы (типа etc\_hosts, ssh\_config, etc), которые используются в дальнейших этапах.
- Настраивает общую подсетку в VirtualBox.
- В цикле создает и запускает headless все 9 виртуалок: 2 ядра, 1 GiB ОЗУ, 5 GiB виртуальный диск.
- Устанавливает на них Linux из указанного образа: один раздел винчестера, без swap. Переключение раскладок клавиатуры на **Ctrl-Shift**, пароль у root *"changeme"* (полезен на случай захода через консоль). Хоть и будет писаться, что создаётся еще пользователь vboxuser, он не создаётся. При инсталяции записывается публичный ключ root'а для ssh (тот что указан вторым аргументом).
- Запускает, записывает ssh публичный ключ виртуалок на хост.
- Отключает kdump (экономим ресурсы).
- Устанавливает русскую локаль (с английским `LC_MESSAGES`).
- Устанавливает пакеты.
- Выключает виртуалки и делает снэпшот `install`.

Время выполнения 34 минуты на MacBook Pro, время выполнения сильно зависит от количества пакетов требующих обновления. Обратный скрипт `setup/destroy_vms`.

## Настройка pacemaker и установка БД heartbeat
Скрипт `setup/setup`:
- Откатывает к снэпшоту `install`.
- Копирует на все виртуалки нужные файлы.
- Настраивает ntpd, для устойчивой работы при сетевой изоляции, например, на случай стенда на ноутах.
- На всех запускает pcsd: REST демон для `pcs`, редхатовская управлялка Pacemaker.
- На *witness* запускает кворум девайс.
- Создает в pacemaker три кластера, проводит первичную настройку.
- Создает 4 БД (если точнее, кластера PostgreSQL), в первом кластере 2 БД, в остальных по одной.
- PAF модуль (pgsqlms) требует, чтобы до запуска через pacemaker, PostgreSQL был запущен в ручном режиме с установившейся репликацией.
- В Pacemaker создает ресурсы в соответствии к созданным DB, прописывает для них плавающие IP.
- Ждет появления пинга на плавающий IP мастера (как критерий, что кластер встал).
- Создает пользователя *heartbeat* и БД.
- Выключает кластера, виртуалки, делает снэпшот `setup`.

Время выполнения 9 минуты на MacBook Pro. Обратный скрипт `setup/rollback2install`. И чтобы откатиться на момент завершения скрипта `setup` скрипт `setup/rollback2setup`. Шелловские скрипты для тестирования доступности этих БД лежат в директории hearbeat (запускаются на хосте).

## Всё вместе
Для удобства сделал скрипт `setup/make_all`, который последовательно выполняет все вышеописанные скрипты, предворяя их `destroy_vms`. Должны быть указаны аргументы такие же, как и у `install`.

# Конфиги на хосте
Для удобства своей работы, я внес следующие изменения в конфиги на хосте.

#### /etc/hosts
Добавил строчки:

	# Tuchanka
	192.168.89.1	tuchanka1a tuchanka1a.tuchanka
	192.168.89.2	tuchanka1b tuchanka1b.tuchanka
	192.168.89.11	tuchanka2a tuchanka2a.tuchanka
	192.168.89.12	tuchanka2b tuchanka2b.tuchanka
	192.168.89.21	tuchanka4a tuchanka4a.tuchanka
	192.168.89.22	tuchanka4b tuchanka4b.tuchanka
	192.168.89.23	tuchanka4c tuchanka4c.tuchanka
	192.168.89.24	tuchanka4d tuchanka4d.tuchanka
	192.168.89.251	witness witness.tuchanka
	192.168.89.101	krogan1a krogan1a.tuchanka
	192.168.89.102	krogan1b krogan1b.tuchanka
	192.168.89.103	krogan2 krogan2.tuchanka
	192.168.89.104	krogan2s1 krogan2s1.tuchanka
	192.168.89.105	krogan4 krogan4.tuchanka
	192.168.89.106	krogan4s1 krogan4s1.tuchanka
	192.168.89.107	krogan4s2 krogan4s2.tuchanka
	192.168.89.108	krogan4s3 krogan4s3.tuchanka
	192.168.89.254	virtualbox virtualbox.tuchanka

#### ~/.ssh/config
Добавил строчки:

	#	Tuchanka
	Host tuchanka1a
		HostName 192.168.89.1
	Host tuchanka1b
		HostName 192.168.89.2
	Host tuchanka2a
		HostName 192.168.89.11
	Host tuchanka2b
		HostName 192.168.89.12
	Host tuchanka4a
		HostName 192.168.89.21
	Host tuchanka4b
		HostName 192.168.89.22
	Host tuchanka4c
		HostName 192.168.89.23
	Host tuchanka4d
		HostName 192.168.89.24
	Host witness
		HostName 192.168.89.251
	Host  tuchanka1a tuchanka1b tuchanka2a tuchanka2b tuchanka4a tuchanka4b tuchanka4c tuchanka4d witness
		ForwardAgent yes
		ForwardX11 no
		AddKeysToAgent yes
		AddressFamily inet
		BindAddress 192.168.89.254
		CanonicalizeHostname no
		CheckHostIP yes
		Compression no
		HashKnownHosts no
		StrictHostKeyChecking yes
		TCPKeepAlive yes
		ServerAliveInterval 5
		User root
		UserKnownHostsFile /Users/!my_user!/prog/domclick/tuchanka/ssh_known_hosts

В последнюю строку надо будет записать реальный путь к файлу (файл создается при первом запуске `install`).

# Тестирование
Переключение раскладок клавиатуры на виртуалках (в виртуальных консолях) **Ctrl-Shift**, пароль у root "**changeme**" (полезен на случай захода через консоль). Но удобнее работать через `ssh` с аутентификацией через уже заданный в скрипте `install` публичный ключ.

Поскольку все виртуалки объеденены в группы, согласно кластерам, запускать и останавливать группу в приложении VirtualBox можно через правую кнопку мыши на имени группы(кластера). Запускать либо целиком группу *Tuchanka*, либо запуская *Tuchanka0* и нужную подгруппу, например *Tuchanka1*. После этого зайти на любую машину нужного кластера и выполнить команду:

	pcs cluster start --all

Кластер автоматический не поднимается (сервер не добавляется в кластер при загрузке) потому, что, по идее, если машина перезагрузилась или включилась, то сисадмин должен проверить причину, устранить, синхронизировать БД и после этого вручную добавить машину в работающий кластер. Если бы машина добавлялась в кластер автоматический при загрузке, то тогда была бы возможна ситуация когда машина постоянно перезагружается, да еще мешает работе кластера.

Внутри виртуалки, находящейся в кластере, состояние кластера можно мониторить скриптом `mon`, находится в /root/bin. Востанавливать БД, после того как она рассинхронизировалась, можно скриптами типа `restore1a`, лежат в /root/bin. Файлы типа `restore1a` или `restore2` только удаляют текущую директорию БД и копируют БД с текущего мастера. А файлы `restore` (созданы только для тестового стенда) выполняют рутинную работу при тестировании: запускают файл типа `restore1a`, добавляют ноду в кластер `pcs cluster start` и удаляют сообщения о старых ошибках из кластера `pcs resource cleanup`. На продакшине использовать такие скрипты считаю нецелесообразным, т.к. если причина отказа работы сервера неизвeстна, то сисадмин должен контролировать каждый шаг.

## Отключение всех машин в кластере
Жесткое отключение - на заголовке группы и на witness *Close/Power Off*. Если хотите мягко, то сначала на одной из машин кластера выполнить:

	pcs cluster stop --all --wait

Потом на заголовке группы и на witness *Close/ACPI Shutdown*. В VirtualBox **6.0.10** при этом иногда всплывает окно с сообщением об ошибке, но, вроде, на корректную работу это никак не влияет.

## Heartbeat
Скриптом `setup` была установлена БД *heartbeat*, можно использовать скрипты из папки `heartbeat/` (на хосте). Все скрипты коннектятся к плавающим IP, на которых оказываются услуги и пишут/читают текущее время в таблицу heartbeat (состоящую из одной строчки, одного столбца) с частой 0.1 секунды (в идеале).

Все скрипты начинающиеся на heart пишут в плавающие IP принадлежащие мастеру соответствующей БД. Скрипты которые начинаются на reader читают из плавающих IP принадлежащих рабам соответствующей БД, за исключением tuchanka1 (т.к. по схеме с уплотнением рабы услуги не оказывают , плавающих IP у рабов нет).

Чтобы все это работало, на хосте в /etc/hosts должны быть добавлены плавающие IP, как написано выше. Так же должен быть установлен `psql`, например через `brew install postgresql`.

## Пример теста
Пример теста на обесточивание сервера/датацентра.
- Запуск групп виртуалок *Tuchanka0* и *Tuchanka2*
- Открываем на хосте 4 терминалки.
- В первой: `ssh tuchanka2b`
	- tuchanka2b: `pcs cluster start --all` *(запуск кластера)*
	- tuchanka2b: `mon` *(мониторинг кластера, когда он поднимется)*
- Должна наблюдаться картина:
	- мастер на одном узле,
	- IP мастера (ресурс krogan2IP) там же,
	- раб на другом,
	- его IP (ресурс krogan2sIP) там же.
- Во второй, на хосте: `heartbeat/heart2` *(мониторинг записи в мастер)*
- В третьей, на хосте: `heartbeat/reader2` *(мониториг чтения из раба)*
- В четвертой терминалке, на хосте: `VBoxManage controlvm tuchanka2a poweroff` *("обесточивание" виртуалки)*
- Наблюдаем как это работает. Сначала потупит по таймаутам, потом, в результате должно получится так, что мастер переезжает на единственно оставшуюся виртуалку и оба плавающих IP (мастера и раба) переезжают на неё же. Во-втором и третьем терминале видим, что работа с БД возобновилась.

Восстанавливаем работу кластера:
- Включаем обесточенную виртуаку, в четвертом терминале: `VBoxManage startvm tuchanka2a`
- Ждем когда загрузится и поднимется ssh.
- В четвертом терминале: `ssh tuchanka2a`.
	- tuchanka2a: `restore2` *(копирует текущую БД с мастера)*
	- tuchanka2a: `pcs cluster start` *(добавляет ноду в кластер)*
- Наблюдаем как на в прошлом выключенной виртуалке поднимается раб и рабский IP переезжает на него.

Выключение кластера:
- После чего в управлялке через правую мышу *Close/Power Off* на группе *Tuchanka2* и *Tuchanka0*.

Другие тесты будут отличаться способом, каким имитируется неисправность, и спецификой работы каждого кластера. Например, обрыв сети можно имитировать командой:\
`VBoxManage controlvm tuchanka2a setlinkstate1 off`\
Только надо потом (перед restore) не забыть "починить" сеть командой:\
`VBoxManage controlvm tuchanka2a setlinkstate1 on`

Проверял следующие отказы:
- Потеря питания: `VBoxManage controlvm tuchanka2a poweroff`
- Потеря линка: `VBoxManage controlvm tuchanka2a setlinkstate1 off`
- Крэш PostgreSQL (master и slave): `kill -KILL`

И следующие административные команды: disable, enable, restart, ban, move, clear (не забывать делать после ban и move), standby, unstandby и т.д. (Подробности в документации к *pacemaker*).

## Известные недостатки
На текущий момент *watchdog демон* *sbd* отрабатывает остановку наблюдаемых демонов, но не их зависание. И, как следствие, некорректно отрабатываются неисправности приводящие к зависанию только *corosync* и *pacemaker*, но при этом не подвешивающие *sbd*. Для проверки *corosync* уже есть **PR#83** (в GitHub у *sbd*) принят в ветку master. Обещали (в PR#83), что и для pacemaker будет что-то подобное, надеюсь, что к *RedHat 8* сделают. Но подобные "неисправности" умозрительные, легко имитируется искусствено с помощью, например `killall -STOP corosync`, но никогда не встречались естественно.
